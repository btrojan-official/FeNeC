{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from model import FeNeC\n",
    "\n",
    "from configs.imagenet_subset_resnet import config\n",
    "from utils.loader import FeNeCDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training and evalutating loop\n",
    "\n",
    "For simplicity, you can pass extracted features from backbone with shapes as below to the model:\n",
    "- X_train (num_train_samples, num_features)\n",
    "- y_train (num_train_samples)\n",
    "- X_test (num_test_samples, num_features)\n",
    "- y_test (num_test_samples)\n",
    "\n",
    "Here we are using our dataloader, but it requires the datasets to be stored in\n",
    "the given data directory and the directory with dataset name (ex. \"./data/tinyimagenet\") Each task data need to be stored\n",
    "in task_0.hdf5 files were 0 is number of task. In each of hdf5 files there are some keys specified. \n",
    "Look at utils/loader.py file for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 - Accuracy: 0.8121910691261292\n",
      "Task 1 - Accuracy: 0.7651376128196716\n",
      "Task 2 - Accuracy: 0.7211764454841614\n",
      "Task 3 - Accuracy: 0.7169323563575745\n",
      "Task 4 - Accuracy: 0.6973500847816467\n",
      "Task 5 - Accuracy: 0.6715887188911438\n",
      "Task 6 - Accuracy: 0.6632136106491089\n",
      "Task 7 - Accuracy: 0.6534972786903381\n",
      "Task 8 - Accuracy: 0.644777774810791\n",
      "Task 9 - Accuracy: 0.6371357440948486\n",
      "\n",
      "Average incremental accuracy: 0.6983000695705414\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "num_tasks = 6\n",
    "num_classes = 100\n",
    "accuracies = []\n",
    "\n",
    "data_loader = FeNeCDataLoader(\n",
    "    num_tasks=num_tasks,\n",
    "    dataset_name=\"imagenet_subset_path\",\n",
    "    load_covariances=True,\n",
    "    load_prototypes=False,\n",
    "    dataset_path=\"./data\",\n",
    ")\n",
    "model = FeNeC(config, device=device)\n",
    "\n",
    "# Matrix to hold per-task per-class accuracy\n",
    "per_class_accuracy_matrix = np.zeros((num_tasks, num_classes))\n",
    "\n",
    "for task_id in range(num_tasks):\n",
    "    X_train, y_train, X_test, y_test, covariances, prototypes = data_loader.get_data(task_id)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Overall accuracy\n",
    "    accuracy = (\n",
    "        torch.sum((y_test.flatten().to(device) == predictions).int()) / X_test.shape[0]\n",
    "    ).item()\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Task {task_id} - Accuracy: {accuracy}\")\n",
    "\n",
    "    # Per-class accuracy\n",
    "    y_test_cpu = y_test.flatten().cpu()\n",
    "    preds_cpu = predictions.cpu()\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        mask = y_test_cpu == cls\n",
    "        if mask.sum().item() == 0:\n",
    "            acc = np.nan  # No samples for this class\n",
    "        else:\n",
    "            correct = (preds_cpu[mask] == cls).sum().item()\n",
    "            acc = correct / mask.sum().item()\n",
    "\n",
    "        per_class_accuracy_matrix[task_id, cls] = acc\n",
    "\n",
    "# Print average accuracy\n",
    "print(f\"\\nAverage incremental accuracy: {sum(accuracies)/len(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_class_accuracy_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.7522401866910995\n"
     ]
    }
   ],
   "source": [
    "num_classes_task_0 = 50\n",
    "num_classes_per_task = 10\n",
    "bwt_list = [np.mean(per_class_accuracy_matrix[-1][:num_classes_task_0]) - np.mean(per_class_accuracy_matrix[0][:num_classes_task_0])]\n",
    "\n",
    "for i in range(1, num_tasks-1):\n",
    "    bwt_list.append(np.mean(per_class_accuracy_matrix[-1][i*num_classes_per_task + num_classes_task_0 - num_classes_per_task: i*num_classes_per_task + num_classes_task_0]) - np.mean(per_class_accuracy_matrix[i][i*num_classes_per_task + num_classes_task_0 - num_classes_per_task: i*num_classes_per_task + num_classes_task_0]))\n",
    "\n",
    "bwt = np.mean(bwt_list) * 100\n",
    "print(bwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_from_kris = [[0.840],\n",
    "# [0.8064,0.576],\n",
    "# [0.793,0.536,0.601],\n",
    "# [0.780,0.495,0.577,0.536],\n",
    "# [0.7676,0.481,0.553,0.521,0.511],\n",
    "# [0.7584,0.453,0.547,0.501,0.502,0.561]]\n",
    "\n",
    "# bwt = 0\n",
    "# for i in range(len(data_from_kris)-1):\n",
    "#     bwt += data_from_kris[-1][i] - data_from_kris[i][-1]\n",
    "# bwt /= 5\n",
    "\n",
    "# print(bwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWT ResNet CIFAR-100: -6.051999999999998\n",
      "BWT ResNet TinyImageNet: -6.912033153448428\n",
      "BWT ResNet Imagenet-Subset: 1\n",
      "BWT ViT CIFAR: -4.444444444444437\n",
      "BWT ViT ImageNet-R: -6.077016168021989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"BWT ResNet CIFAR-100: {-6.051999999999998}\")\n",
    "print(f\"BWT ResNet TinyImageNet: {-6.912033153448428}\")\n",
    "print(f\"BWT ResNet Imagenet-Subset: {1}\")\n",
    "print(f\"BWT ViT CIFAR: {-4.444444444444437}\")\n",
    "print(f\"BWT ViT ImageNet-R: {6.7522401866910995}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
